{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  IPE PYTHON II_Paula Escusol Entío\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, \\\n",
    "    matthews_corrcoef, precision_score, confusion_matrix, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge missingno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserting and readingg the data (excel file)\n",
    "df = pd.read_excel(\"AUTO_LOANS_DATA.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Basic steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Change the name of the columns to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want to transform the name of all the columns to lower case\n",
    "df.rename(columns=lambda x: x.lower(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Missing not at random analysis -  Analyzing missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the distribution of the missing values\n",
    "msno.matrix(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use the heatmap to identify if there is a relationship in the presence of null values between the columns:\n",
    "    #Values close to positive 1 indicate that the presence of null values in one column is correlated with the presence of null values in another column.\n",
    "    #Values close to negative 1 indicate that the presence of null values in one column is anti-correlated with the presence of null values in another column.\n",
    "    #Values close to 0, indicate there is little to no relationship between the presence of null values in one column compared to another.\n",
    "msno.heatmap(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.dendrogram(df)\n",
    "#In the dendrogram plot below, we can see we have three distinct groups:\n",
    "    #The first is on the right side (profession, sex, and birth_date) which all have a high degree of null values;\n",
    "    \n",
    "    #The second is on the left, with the remainder of the columns which are more complete (customer_open_date,\n",
    "    #bucket, outstanding, original_booked_amoount...) are all grouped together at zero indicating that they\n",
    "    #are complete.\n",
    "    \n",
    "    #The variable car_type also has a high degree of high values. However, as we have seen in the heatmap above, \n",
    "    #the presence of null values in the car_type variable is anti-correlated with the presence of null values in \n",
    "    #any other column. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Removing duplicate records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let’s explore if there are any duplicate values in the dataset:\n",
    "df['customer_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that some of the records (customers) are recorded multiple times. In case we decided to remove the\n",
    "# duplicate records, our approach would be to keep only the last record for each customer, as follows:\n",
    "    #df.sort_values(by=['loan_open_date'])\n",
    "    #df.drop_duplicates('customer_id', keep = 'last', inplace = True)\n",
    "    \n",
    "#However, to be able to analyze the accuracy of our model when splitting the dataset (refer to section \"Risk Based\n",
    "#Segmentation by variable\" of the document) we have decided not to remove the duplicate records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REMOVE - 2. Creating a new column \"user type\" to identify the type of user: Corporate (1) vs. Individuals (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicating the column \"program_name\" to a new column \"user_type\" to later identify the type of the user: corporates or individuals\n",
    "df['user_type'] = df.apply(lambda row: row.program_name, axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"user_type\"].replace({\"Auto Loans Corporate Guarantee\": \"1\", \n",
    "                         \"Auto Loans 50% Down Payment - Employed\": \"0\",\n",
    "                        \"Pick Up and Small Trucks\": \"0\",\n",
    "                        \"Auto Loans 40% Down Payment - Employed\": \"0\",\n",
    "                        \"Auto Loans 30% Down Payment - Self Employed\": \"0\",\n",
    "                        \"Auto Loans 40% Down Payment - Self Employed\": \"0\",\n",
    "                        \"Auto Loans 20% Down Payment - Employed\": \"0\",\n",
    "                        \"Auto Loans 30% Down Payment - Employed\": \"0\",\n",
    "                        \"Auto Loans 50% Down Payment - Self Employed\": \"0\",\n",
    "                        \"Auto Loans Special Deals\": \"0\",\n",
    "                        \"Auto Loans Payroll Clients\": \"0\",\n",
    "                        \"Auto Loans Secured against CD\": \"0\",\n",
    "                        \"Auto Loans Doctors - Employed\": \"0\",\n",
    "                        \"Auto Loans Fully Secured\": \"0\",\n",
    "                        \"Auto Loans Doctors - Self Employed\": \"0\",\n",
    "                        \"Auto Loans 20% Down Payment - Self Employed\": \"0\",\n",
    "                        \"Auto Loans 50% Down Payment No Car Prohibition - Self Employed\": \"0\",\n",
    "                        \"Auto Loans Run Off - Self Employed\":\"0\",\n",
    "                        \"Auto Loans Run Off - Employed\": \"0\",\n",
    "                        \"Auto Loans 50% Down Payment No Car Prohibition - Employed\": \"0\",\n",
    "                        \"Auto Loans 50% Down Payment Used Cars - Self Employed\": \"0\",\n",
    "                        \"Auto Loans 50% Down Payment Used Cars - Employed\": \"0\",\n",
    "                        \"Auto Loans 30% Down Payment Used Cars - Self Employed\": \"0\",\n",
    "                        \"Auto Loans 30% Down Payment Used Cars - Employed\": \"0\"}, inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of individuals (0) vs. corporates (1)\n",
    "df.user_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we should "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Transforming the variable \"program_name\" into a categorical variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function below transforms the variable \"program_name\" into a categorical variable.\n",
    "\n",
    "df['program_name'] = df.program_name.astype(\"category\").cat.codes\n",
    "df.program_name, df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"program_name\"] = df[\"program_name\"].astype(\"category\")\n",
    "df.program_name, df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_type = ['Categorical' if x.name == 'category' else 'Numerical' for x in df.dtypes]\n",
    "feat_type\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Transforming the variable \"birth_date\" into age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'] = pd.to_datetime(df['birth_date'], format='%Y-%m-%d')\n",
    "df['age'] = ((df.reporting_date - df.birth_date)/np.timedelta64(1, 'Y'))\n",
    "\n",
    "df.round({\"age\":0})\n",
    "\n",
    "df.drop('birth_date', inplace=True, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Removing Nans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Selecting only the numerics to replace NaNs or blanks with 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check the amount of missing values that we have in each variable\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_numeric_columns = ['age']\n",
    "for item in list_numeric_columns:\n",
    "    df[item] = df[item].fillna(0).replace('NaN',0).replace('',0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Selecting non-numerics to replace NaNs with blanks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within the non-numeric variables with missing values, we can distinguish two groups: \n",
    "    #1 - Non-numerics which have missing values related to the type of user (MNAR): Corporate users do not contain information\n",
    "    #regarding the sex or profession.\n",
    "    \n",
    "    #2 - Non-numerics with missing at random data (MAR)- concretelly, the variable CAR_TYPE.\n",
    "    \n",
    "#We will deal with these situations differently: \n",
    "\n",
    "#1 - MNAR for Corporate users: \n",
    "       \n",
    "list_non_numeric_columns = ['profession', 'sex']\n",
    "for item in list_non_numeric_columns:\n",
    "    df[item] = df[item].fillna('').replace('NaN','');\n",
    "    \n",
    "#2 - MAR for CAR_TYPE:\n",
    "    #Once the variable CAR_TYPE is converted to a categorical variable, we will replace the missing values with \n",
    "    #the most common value of the variable (the mode). Refer to point \"12 - Transforming the variable \"car_type\" \n",
    "    #to categorical\" to see this point.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Transforming the variable \"loan_open_date\" into months that the row has been opened at the reporting date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts a given date (loan_open_date) to the number of months that the contract has been opened \n",
    "# at each reporting date.\n",
    "                                   \n",
    "df['months_loan_opened'] = ((df.reporting_date - df.loan_open_date)/np.timedelta64(1, 'M'))\n",
    "df.drop('loan_open_date', inplace=True, axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Transforming the variable \"expected_close_date\" into months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts the \"expected_close_date\" variable into the number of months that the contract will still be opened.\n",
    "\n",
    "df['months_to_close_loan'] = ((df.expected_close_date - df.reporting_date)/np.timedelta64(1, 'M'))\n",
    "df.drop('expected_close_date', inplace=True, axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Transforming the variable \"customer_open_date\" into months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts the \"customer_open_date\" variable into the number of months that the customer has had a \n",
    "# contract has been opened at each reporting date \n",
    "                                   \n",
    "df['months_client_opened'] = ((df.reporting_date - df.customer_open_date)/np.timedelta64(1, 'M'))\n",
    "#df['months_client_opened'] = df['customer_open_date'].astype(int)\n",
    "df.drop('customer_open_date', inplace=True, axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Transforming the variable \"sex\" into dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the scales of the variable \"sex\" to 0 and 1, respectively.\n",
    "\n",
    "# Get one hot encoding of column sex\n",
    "one_hot = pd.get_dummies(df['sex'])\n",
    "# Drop column sex as it is now encoded\n",
    "df = df.drop('sex',axis = 1)\n",
    "# Join the encoded df\n",
    "df = df.join(one_hot)\n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Transforming the target variable \"bucket\" into dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variable \"bucket\" indicates the number of unpaid installements at each reporting date. We have transformed\n",
    "# this variable to dummy variables to indicate whether the user has had unpaid installments (1) or not(0).\n",
    "bucket_replace = {2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1}\n",
    "df = df.replace({'bucket': bucket_replace})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bucket'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Merging the variable \"profession\" and tranforming it into a categorical variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let’s explore in how many different professions we have in the dataset:\n",
    "\n",
    "df['profession'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#First, lets transform all the scales of the \"profession\" variable to lower case\n",
    "df[\"profession\"] = df[\"profession\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, since we have a lot of unneccesary categories, we will group all the scales into new categories and change \n",
    "#the type of the variable to categorical.\n",
    "#The new categories that we will create are based on the existing categories, and will be: \n",
    "    #Employee (includes, among others, religious persons, military officers, politicians and athletes)\n",
    "    #Company or shop owner\n",
    "    #Homemaker (before: \"housewife\")\n",
    "    #Retired\n",
    "    #Unemployed\n",
    "    #Landlord\n",
    "    #Student\n",
    "    \n",
    "profession_replace = {\"employee\": \"Employee\", \n",
    "                          \"company owner\": \"Company or shop owner\", \n",
    "                         \"manager\": \"Employee\",\n",
    "                        \"shop owner\": \"Company or shop owner\",\n",
    "                       \"instructor / teacher\": \"Employee\",\n",
    "                          \"housewife\": \"Homemaker\",\n",
    "                          \"accountant - employee\": \"Employee\",\n",
    "                          \"engineer\": \"Employee\",\n",
    "                          \"doctor\": \"Employee\",\n",
    "                          \"retired\": \"Retired\",\n",
    "                          \"unemployed\": \"Unemployed\",\n",
    "                          \"contractor\": \"Employee\",\n",
    "                          \"pharmacist\": \"Employee\",\n",
    "                          \"nurse\": \"Employee\",\n",
    "                          \"technician\": \"Employee\",\n",
    "                          \"secretary\": \"Employee\",\n",
    "                          \"business man / trader\": \"Employee\",\n",
    "                          \"professors\": \"Employee\",\n",
    "                          \"landlord\": \"Landlord\",\n",
    "                          \"banker\": \"Employee\",\n",
    "                          \"driver\": \"Employee\",\n",
    "                          \"journalist\": \"Employee\",\n",
    "                          \"chemist\": \"Employee\",\n",
    "                          \"tour leader\": \"Employee\",\n",
    "                          \"jeweller\": \"Employee\",\n",
    "                          \"religion person\": \"Employee\",\n",
    "                          \"artist\": \"Employee\",\n",
    "                          \"broadcast / media\": \"Employee\",\n",
    "                          \"athletes\": \"Employee\",\n",
    "                          \"lawyer ? self employer\": \"Employee\",\n",
    "                          \"craftsman\": \"Employee\",\n",
    "                          \"car / boat agency / deale\": \"Employee\",\n",
    "                          \"student\": \"Student\",\n",
    "                          \"consultant\": \"Employee\",\n",
    "                          \"hostess\": \"Employee\",\n",
    "                          \"diver\": \"Employee\",\n",
    "                          \"lawyer ? employee\": \"Employee\",\n",
    "                          \"police officer\": \"Employee\",\n",
    "                          \"bazaar shop owner\": \"Company or shop owner\",\n",
    "                          \"pilot\": \"Employee\",\n",
    "                          \"cae - current staff\": \"Employee\",\n",
    "                          \"military officer\": \"Employee\",\n",
    "                          \"cae ibs staff\": \"Employee\",\n",
    "                          \"real estate broker / agen\": \"Employee\",\n",
    "                          \"agrarian\": \"Employee\",\n",
    "                          \"economist\": \"Employee\",\n",
    "                          \"press\": \"Employee\",\n",
    "                          \"cae ex-staff less than 10\": \"Employee\",\n",
    "                          \"publisher\": \"Employee\",\n",
    "                          \"politician\": \"Employee\",\n",
    "                          \"hotel manager\": \"Employee\"}\n",
    "\n",
    "df = df.replace({'profession': profession_replace})\n",
    "\n",
    "df['profession'] = df['profession'].astype(str)\n",
    "df['profession'].value_counts()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If instead of transforming the variable program_name, we want to create a new variable, we should do: \n",
    "#df['program_name_category'] = df.program_name.astype(\"category\").cat.codes\n",
    "\n",
    "df['profession'] = df.program_name.astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"profession\"] = df[\"profession\"].astype(\"category\")\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html\n",
    "df.profession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_type = ['Categorical' if x.name == 'category' else 'Numerical' for x in df.dtypes]\n",
    "feat_type\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Transforming the variable \"car_type\" into categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we did before, let’s explore in how many different car types we have in the dataset:\n",
    "df['car_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We shall also transform all the scales to lower case\n",
    "df[\"car_type\"] = df[\"car_type\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['car_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we want to group all the scales of the variable \"car_type\" into new categories, and change the type of the\n",
    "#column to a categorical variable.\n",
    "#The new categories that have been created are based on the continent of origin of the cars, and will be: \n",
    "    #Asian cars;\n",
    "    #American cars;\n",
    "    #European cars;\n",
    "    #Others\n",
    "\n",
    "car_type_replace = {'kia': \"asian cars\",\n",
    "                    'carry': \"others\",\n",
    "                    'chevrolet': \"american cars\",\n",
    "                    'mitsubishi': \"asian cars\",\n",
    "                    'seat': \"european cars\",\n",
    "                    'skoda': \"european cars\",\n",
    "                    'renault': \"european cars\",\n",
    "                    'mercedes': \"european cars\",\n",
    "                    'jack': \"asian cars\",\n",
    "                    'byd': \"asian cars\",\n",
    "                    'gely': \"asian cars\",\n",
    "                    'hyundai': \"asian cars\",\n",
    "                    'nissan': \"asian cars\",\n",
    "                    'changan': \"asian cars\",\n",
    "                    'gelory': \"asian cars\",\n",
    "                    'suzuki': \"asian cars\",\n",
    "                    'bmw': \"european cars\",\n",
    "                    'daihatsu': \"asian cars\",\n",
    "                    'ssang yong': \"asian cars\",\n",
    "                    'baic': \"asian cars\",\n",
    "                    'toyota': \"asian cars\",\n",
    "                    'lada': \"others\",\n",
    "                    'mazda': \"asian cars\",\n",
    "                    'brilliance': \"asian cars\",\n",
    "                    'kenbo': \"asian cars\",\n",
    "                    'speranza': \"others\",\n",
    "                    'saipa': \"others\",\n",
    "                    'opel': \"european cars\",\n",
    "                    'peugeot': \"european cars\",\n",
    "                    'chana': \"asian cars\",\n",
    "                    'citroen': \"european cars\",\n",
    "                    'isuzu': \"asian cars\",\n",
    "                    'proton': \"asian cars\",\n",
    "                    'honda': \"asian cars\",\n",
    "                    'volkswagen': \"european cars\",\n",
    "                    'chery': \"others\",\n",
    "                    'fiat': \"european cars\",\n",
    "                    'subaru': \"asian cars\",\n",
    "                    'jeep': \"american cars\",\n",
    "                    'volvo': \"european cars\",\n",
    "                    'mini': \"european cars\",\n",
    "                    'ford': \"american cars\",\n",
    "                    'great wall': \"asian cars\",\n",
    "                    'mg': \"european cars\",\n",
    "                    'mable': \"asian cars\",\n",
    "                    'haima': \"asian cars\",\n",
    "                    'changy': \"asian cars\",\n",
    "                    'audi': \"european cars\",\n",
    "                    'mahindra': \"others\",\n",
    "                    'livan': \"asian cars\",\n",
    "                    'florida': \"others\",\n",
    "                    'dodge': \"american cars\",\n",
    "                    'zemex': \"asian cars\",\n",
    "                    'dfsk': \"asian cars\",\n",
    "                    'saweast': \"asian cars\",\n",
    "                    'zoty': \"asian cars\",\n",
    "                    'faw': \"asian cars\",\n",
    "                    'hawtai': \"asian cars\",\n",
    "                    'jaguar': \"european cars\",\n",
    "                    'victory': \"american cars\"}\n",
    "\n",
    "df = df.replace({'car_type': car_type_replace})\n",
    "\n",
    "#df['car_type'] = df['car_type'].astype(str)\n",
    "df['car_type'].value_counts()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['car_type'] = df.program_name.astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"car_type\"] = df[\"car_type\"].astype(\"category\")\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html\n",
    "df.car_type, df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_type = ['Categorical' if x.name == 'category' else 'Numerical' for x in df.dtypes]\n",
    "feat_type\n",
    "# Then use feat_type in your class: cls.fit(X_train, y_train, X_test, y_test, feat_type=feat_type)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we have previously analyzed, the missing values for the variable \"car_type\" are random and are not related \n",
    "#with the user_type. Therefore, wow that we have converted the variable \"car_type\" into a categorical variable, \n",
    "#we can inpute the mode to complete the missing values: \n",
    "\n",
    "df['car_type'].fillna(df['car_type'].mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Dropping the variables \"account_number\", \"customer_id\" and \"reporting_date\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the aim of the project is to find which variables are good for segmentation, variables that only provide\n",
    "#identification of the client or account (account_number and customer_id) do not provide relevant information and\n",
    "#therefore, we have decided to remove them from the dataset. \n",
    "\n",
    "#Moreover, the variable \"reporting_date\" has also been removed from the dataset as it does not provide usefull\n",
    "#information about any specific segment of the population. \n",
    "\n",
    "del df['account_number']\n",
    "del df['reporting_date']\n",
    "del df['customer_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Creating buckets for the variable \"outstanding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The variable \"outstanding\" is a continue variable which probably holds as many unique values as records we have\n",
    "#in our dataset. In order to be able to calculate the Information Gain of this variable towards the target \n",
    "#variable (bucket), we shall reduce the number of possible values of the \"outstanding\" variable. We will do so\n",
    "#using qcut function to define the number of quantiles and divide up the data. \n",
    "\n",
    "pd.qcut(df['outstanding'], q=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then, we will store the bin results back in the original dataframe \n",
    "\n",
    "df['outstanding_quantile'] = pd.qcut(df['outstanding'], q=50, precision=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To be able to use this information for further analysis, we will transform the variable \"outstanding_cuantile\"\n",
    "#into a categorical variable.\n",
    "\n",
    "df['outstanding_quantile'] = df.program_name.astype(\"category\").cat.codes\n",
    "df[\"outstanding_quantile\"] = df[\"outstanding_quantile\"].astype(\"category\")\n",
    "df.program_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lastly, we can remove the original variable \"outstanding\" from our dataset since we won't be using it.\n",
    "\n",
    "del df['outstanding']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Risk based segmentation: Splitting the data based on the user_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Business segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By analyzing the data, we have seen that there are some variables missing not at random for certain users.\n",
    "#Precisely, we have seen that the dataset contains two types of users: individuals and corporates. We can easily\n",
    "#identify Corporates since these records have common missing values related to demographic information: \n",
    "    #SEX \n",
    "    #BIRTH_DATE (now transformed to AGE)\n",
    "    #PROFESSION\n",
    "#Therefore, we conclude that the first criteria for segmentation (business segmentation or risk segmentation) \n",
    "#shall be based on the user_type: individuals vs. corporates.\n",
    "\n",
    "dfIndividuals, dfCorporates = [x for _, x in df.groupby(df['user_type'] == '1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfIndividuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCorporates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing information gain for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_random_sample, _ = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_random_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'bucket'\n",
    "descriptive_feature = ['program_name', 'original_booked_amount', 'profession', 'car_type', 'user_type', 'age', 'months_loan_opened', 'months_to_close_loan', 'months_client_opened', 'F', 'M', 'outstanding_quantile'],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Information gain is used for determining the best features/attributes that render maximum information about a \n",
    "#target variable.\n",
    "\n",
    "import io\n",
    "import requests\n",
    "\n",
    "class InformationGain():\n",
    "    \n",
    "    def  __init__(self, target, descriptive_feature):\n",
    "         self.target = target\n",
    "         self.descriptive_feature = descriptive_feature\n",
    "        \n",
    "def compute_impurity(feature, impurity_criterion):\n",
    "    \n",
    "    #This function calculates impurity of a feature. Supported impurity criteria: 'entropy', 'gini'.\n",
    "    \n",
    "    probs = feature.value_counts(normalize=True)\n",
    "    \n",
    "    if impurity_criterion == 'entropy':\n",
    "        impurity = -1 * np.sum(np.log2(probs) * probs)\n",
    "    elif impurity_criterion == 'gini':\n",
    "        impurity = 1 - np.sum(np.square(probs))\n",
    "    else:\n",
    "        raise ValueError('Unknown impurity criterion')\n",
    "        \n",
    "    return(round(impurity, 3))    \n",
    "\n",
    "target_entropy = compute_impurity(df[target], 'entropy')\n",
    "target_entropy\n",
    "\n",
    "def comp_feature_information_gain(df1, target, descriptive_feature, split_criterion):\n",
    "    \n",
    "    # This function calculates information gain for splitting on a particular descriptive feature for a given \n",
    "    # dataset and a given impurity criteria.\n",
    "        \n",
    "    print('target variable:', target)\n",
    "    print('descriptive_feature:', descriptive_feature)\n",
    "    print('split criterion:', split_criterion)\n",
    "            \n",
    "    target_entropy = compute_impurity(df1[target], split_criterion)\n",
    "\n",
    "    # Two lists have been defined:\n",
    "        # entropy_list to store the entropy of each partition\n",
    "        # weight_list to store the relative number of observations in each partition\n",
    "    entropy_list = list()\n",
    "    weight_list = list()\n",
    "    \n",
    "    # Loop over each level of the feature to partition the dataset with respect to that level and compute\n",
    "    # the entropy and the weight of the level's partition\n",
    "    \n",
    "    for level in df[descriptive_feature].unique():\n",
    "        df_feature_level = df1[df1[descriptive_feature] == level]\n",
    "        entropy_level = compute_impurity(df_feature_level[target], split_criterion)\n",
    "        entropy_list.append(round(entropy_level, 3))\n",
    "        weight_level = len(df_feature_level) / len(df)\n",
    "        weight_list.append(round(weight_level, 3))\n",
    "\n",
    "    #print('impurity of partitions:', entropy_list)\n",
    "    #print('weights of partitions:', weight_list)\n",
    "\n",
    "    feature_remaining_impurity = np.sum(np.array(entropy_list) * np.array(weight_list))\n",
    "    #print('remaining impurity:', feature_remaining_impurity)\n",
    "    \n",
    "    information_gain = target_entropy - feature_remaining_impurity\n",
    "    print('information gain:', information_gain)\n",
    "    if information_gain <0.1:\n",
    "        print(\"Not good for segmentation. Information Gain ratio not significant for BUCKET variable.\")\n",
    "    else:\n",
    "        print(\"Good for segmentation. Pending further analysis\")\n",
    "    \n",
    "    print('====================')\n",
    "    \n",
    "    return(information_gain)\n",
    "\n",
    "#I have determined that for a variable to be significant for the segmentation, its Information Gain Ratio should be >0.1\n",
    "#Information gain method obtained from: https://www.kaggle.com/edouarddesprez/health-insurance-auc-0-857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_gain_list = list()\n",
    "descriptive_feature_list = list()\n",
    "\n",
    "split_criteria = 'gini'\n",
    "for feature in df.drop(columns='bucket').columns:\n",
    "    feature_info_gain = comp_feature_information_gain(df1, 'bucket', feature, split_criteria)\n",
    "    information_gain_list.append(feature_info_gain)\n",
    "    descriptive_feature_list.append(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression model - all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'bucket'\n",
    "all_variables = ['program_name', 'original_booked_amount', 'profession', 'car_type', 'user_type', 'age', 'months_loan_opened', 'months_to_close_loan', 'months_client_opened', 'F', 'M', 'outstanding_quantile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "splitter = train_test_split\n",
    "\"-----------------------\"\n",
    "\n",
    "df_train, df_test = splitter(df1, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[all_variables]\n",
    "y_train = df_train[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test[all_variables]\n",
    "y_test = df_test[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "method = LogisticRegression(random_state=0)\n",
    "fitted_full_model = method.fit(X_train, y_train)\n",
    "y_pred = fitted_full_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = fitted_full_model.predict_proba(X_test)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GINI Coefficient\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr,tpr,thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "GINI = (2*roc_auc) -1\n",
    "GINI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "#X_train, X_test,y_train, y_test = train_test_split(x,y,test_size=0.2,random_state = 42)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing the accuracy of our model\n",
    "y_pred = logreg.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Based Segmentation by variable: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will be analyzing the accuracy of our model dividing our dataset by the variables with higher information\n",
    "# gain, as ocmputed in the step above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting by age "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(25, 85, 10):\n",
    "#segment 1\n",
    "    df_train_seg1, df_train_seg2 = [x for _, x in df.groupby(df['age'] < i)]\n",
    "    df_test_seg1, df_test_seg2 = [x for _, x in df.groupby(df['age'] < i)]\n",
    "    X_train_seg1 = df_train_seg1[all_variables]\n",
    "    y_train_seg1 = df_train_seg1[target]\n",
    "    X_test_seg1 = df_test_seg1[all_variables]\n",
    "    y_test_seg1 = df_test_seg1[target]\n",
    "    fitted_model_seg1 = method.fit(X_train_seg1, y_train_seg1)\n",
    "    y_pred_seg1 = fitted_model_seg1.predict(X_test_seg1)\n",
    "    y_pred_seg1_fullmodel = fitted_full_model.predict(X_test_seg1)\n",
    "\n",
    "#segment 2\n",
    "    X_train_seg2 = df_train_seg2[all_variables]\n",
    "    y_train_seg2 = df_train_seg2[target]\n",
    "    X_test_seg2 = df_test_seg2[all_variables]\n",
    "    y_test_seg2 = df_test_seg2[target]\n",
    "    fitted_model_seg2 = method.fit(X_train_seg2, y_train_seg2)\n",
    "    y_pred_seg2 = fitted_model_seg2.predict(X_test_seg2)\n",
    "    y_pred_seg2_fullmodel = fitted_full_model.predict(X_test_seg2)\n",
    "\n",
    "#printing results\n",
    "    print (\"     \")\n",
    "    print(\"Variable analyzed: AGE > or < to\", i)\n",
    "    print(\"     SEGMENT 1: Model Developed on Seg 1 (train sample) applied on Seg 1 (test sample):\",accuracy_score(y_test_seg1, y_pred_seg1))\n",
    "    print(\"     SEGMENT 1: Model Developed on Full Population (train sample) applied on Seg 1 (test sample):\",accuracy_score(y_test_seg1, y_pred_seg1_fullmodel))\n",
    "    print (\"     \")\n",
    "    print(\"     SEGMENT 2: Model Developed on Full Population (train sample) applied on Seg 2 (test sample):\",accuracy_score(y_test_seg2, y_pred_seg2_fullmodel))\n",
    "    print(\"     SEGMENT 2: Model Developed on Seg 2 (train sample) applied on Seg 2 (test sample):\",accuracy_score(y_test_seg2, y_pred_seg2))\n",
    "    print (\"     \")\n",
    "    print (\"     ==============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting by original_booked_amount "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500000, 2000000, 500000):\n",
    "#segment 1\n",
    "    df_train_seg1, df_train_seg2 = [x for _, x in df.groupby(df['original_booked_amount'] < i)]\n",
    "    df_test_seg1, df_test_seg2 = [x for _, x in df.groupby(df['original_booked_amount'] < i)]\n",
    "    X_train_seg1 = df_train_seg1[all_variables]\n",
    "    y_train_seg1 = df_train_seg1[target]\n",
    "    X_test_seg1 = df_test_seg1[all_variables]\n",
    "    y_test_seg1 = df_test_seg1[target]\n",
    "    fitted_model_seg1 = method.fit(X_train_seg1, y_train_seg1)\n",
    "    y_pred_seg1 = fitted_model_seg1.predict(X_test_seg1)\n",
    "    y_pred_seg1_fullmodel = fitted_full_model.predict(X_test_seg1)\n",
    "\n",
    "#segment 2\n",
    "    X_train_seg2 = df_train_seg2[all_variables]\n",
    "    y_train_seg2 = df_train_seg2[target]\n",
    "    X_test_seg2 = df_test_seg2[all_variables]\n",
    "    y_test_seg2 = df_test_seg2[target]\n",
    "    fitted_model_seg2 = method.fit(X_train_seg2, y_train_seg2)\n",
    "    y_pred_seg2 = fitted_model_seg2.predict(X_test_seg2)\n",
    "    y_pred_seg2_fullmodel = fitted_full_model.predict(X_test_seg2)\n",
    "\n",
    "#printing results\n",
    "    print (\"     \")\n",
    "    print(\"Variable analyzed: ORIGINAL_BOOKED_AMOUNT > or < to\", i)\n",
    "    print(\"     SEGMENT 1: Model Developed on Seg 1 (train sample) applied on Seg 1 (test sample):\",accuracy_score(y_test_seg1, y_pred_seg1))\n",
    "    print(\"     SEGMENT 1: Model Developed on Full Population (train sample) applied on Seg 1 (test sample):\",accuracy_score(y_test_seg1, y_pred_seg1_fullmodel))\n",
    "    print (\"     \")\n",
    "    print(\"     SEGMENT 2: Model Developed on Full Population (train sample) applied on Seg 2 (test sample):\",accuracy_score(y_test_seg2, y_pred_seg2_fullmodel))\n",
    "    print(\"     SEGMENT 2: Model Developed on Seg 2 (train sample) applied on Seg 2 (test sample):\",accuracy_score(y_test_seg2, y_pred_seg2))\n",
    "    print (\"     \")\n",
    "    print (\"     ==============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting by months_loan_opened "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12, 60, 12):\n",
    "#segment 1\n",
    "    df_train_seg1, df_train_seg2 = [x for _, x in df.groupby(df['months_loan_opened'] < i)]\n",
    "    df_test_seg1, df_test_seg2 = [x for _, x in df.groupby(df['months_loan_opened'] < i)]\n",
    "    X_train_seg1 = df_train_seg1[all_variables]\n",
    "    y_train_seg1 = df_train_seg1[target]\n",
    "    X_test_seg1 = df_test_seg1[all_variables]\n",
    "    y_test_seg1 = df_test_seg1[target]\n",
    "    fitted_model_seg1 = method.fit(X_train_seg1, y_train_seg1)\n",
    "    y_pred_seg1 = fitted_model_seg1.predict(X_test_seg1)\n",
    "    y_pred_seg1_fullmodel = fitted_full_model.predict(X_test_seg1)\n",
    "\n",
    "#segment 2\n",
    "    X_train_seg2 = df_train_seg2[all_variables]\n",
    "    y_train_seg2 = df_train_seg2[target]\n",
    "    X_test_seg2 = df_test_seg2[all_variables]\n",
    "    y_test_seg2 = df_test_seg2[target]\n",
    "    fitted_model_seg2 = method.fit(X_train_seg2, y_train_seg2)\n",
    "    y_pred_seg2 = fitted_model_seg2.predict(X_test_seg2)\n",
    "    y_pred_seg2_fullmodel = fitted_full_model.predict(X_test_seg2)\n",
    "\n",
    "#printing results\n",
    "    print (\"     \")\n",
    "    print(\"Variable analyzed: MONTHS_LOAN_OPENED > or < to\", i)\n",
    "    print(\"     SEGMENT 1: Model Developed on Seg 1 (train sample) applied on Seg 1 (test sample):\",accuracy_score(y_test_seg1, y_pred_seg1))\n",
    "    print(\"     SEGMENT 1: Model Developed on Full Population (train sample) applied on Seg 1 (test sample):\",accuracy_score(y_test_seg1, y_pred_seg1_fullmodel))\n",
    "    print (\"     \")\n",
    "    print(\"     SEGMENT 2: Model Developed on Full Population (train sample) applied on Seg 2 (test sample):\",accuracy_score(y_test_seg2, y_pred_seg2_fullmodel))\n",
    "    print(\"     SEGMENT 2: Model Developed on Seg 2 (train sample) applied on Seg 2 (test sample):\",accuracy_score(y_test_seg2, y_pred_seg2))\n",
    "    print (\"     \")\n",
    "    print (\"     ==============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting by months_client_opened "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12, 60, 12):\n",
    "#segment 1\n",
    "    df_train_seg1, df_train_seg2 = [x for _, x in df.groupby(df['months_client_opened'] < i)]\n",
    "    df_test_seg1, df_test_seg2 = [x for _, x in df.groupby(df['months_client_opened'] < i)]\n",
    "    X_train_seg1 = df_train_seg1[all_variables]\n",
    "    y_train_seg1 = df_train_seg1[target]\n",
    "    X_test_seg1 = df_test_seg1[all_variables]\n",
    "    y_test_seg1 = df_test_seg1[target]\n",
    "    fitted_model_seg1 = method.fit(X_train_seg1, y_train_seg1)\n",
    "    y_pred_seg1 = fitted_model_seg1.predict(X_test_seg1)\n",
    "    y_pred_seg1_fullmodel = fitted_full_model.predict(X_test_seg1)\n",
    "\n",
    "#segment 2\n",
    "    X_train_seg2 = df_train_seg2[all_variables]\n",
    "    y_train_seg2 = df_train_seg2[target]\n",
    "    X_test_seg2 = df_test_seg2[all_variables]\n",
    "    y_test_seg2 = df_test_seg2[target]\n",
    "    fitted_model_seg2 = method.fit(X_train_seg2, y_train_seg2)\n",
    "    y_pred_seg2 = fitted_model_seg2.predict(X_test_seg2)\n",
    "    y_pred_seg2_fullmodel = fitted_full_model.predict(X_test_seg2)\n",
    "\n",
    "#printing results\n",
    "    print (\"     \")\n",
    "    print(\"Variable analyzed: MONTHS_CLIENT_OPENED > or < to\", i)\n",
    "    print(\"     SEGMENT 1: Model Developed on Seg 1 (train sample) applied on Seg 1 (test sample):\",accuracy_score(y_test_seg1, y_pred_seg1))\n",
    "    print(\"     SEGMENT 1: Model Developed on Full Population (train sample) applied on Seg 1 (test sample):\",accuracy_score(y_test_seg1, y_pred_seg1_fullmodel))\n",
    "    print (\"     \")\n",
    "    print(\"     SEGMENT 2: Model Developed on Full Population (train sample) applied on Seg 2 (test sample):\",accuracy_score(y_test_seg2, y_pred_seg2_fullmodel))\n",
    "    print(\"     SEGMENT 2: Model Developed on Seg 2 (train sample) applied on Seg 2 (test sample):\",accuracy_score(y_test_seg2, y_pred_seg2))\n",
    "    print (\"     \")\n",
    "    print (\"     ==============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution summary report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing not at random report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The variables PROFESSION, SEX and BIRTHDATE (converted to AGE) seem Missing Not at Random, therefore we recommend:\")\n",
    "print('\\033[1m Thin File Segment Variables: \\033[0m',\"REPORTING_DATE, ACCOUNT_NUMBER, CUSTOMER_ID, PROGRAM_NAME, LOAN_OPEN_DATE, EXPECTED_CLOSE_DATE, ORIGINAL_BOOKED_AMOUNT, OUTSTANDING, CUSTOMER_OPEN_DATE, CAR_TYPE\")\n",
    "print('     ')\n",
    "print('\\033[1m Full File Segment Variables: \\033[0m',\"REPORTING_DATE, ACCOUNT_NUMBER, CUSTOMER_ID, PROGRAM_NAME, LOAN_OPEN_DATE, EXPECTED_CLOSE_DATE, ORIGINAL_BOOKED_AMOUNT, OUTSTANDING, SEX, CUSTOMER_OPEN_DATE, BIRTH_DATE, PROFESSION, CAR_TYPE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable by Variable Risk Based Segmentation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m REPORTING_DATE \\033[0m Not good for segmentation. Afer analysis, we did not find a good split using this variable.\")\n",
    "print (\"     \")\n",
    "print (\"\\033[1m ACCOUNT_NUMBER \\033[0m Not good for segmentation. Afer analysis, we did not find a good split using this variable.\")\n",
    "print (\"     \")\n",
    "print (\"\\033[1m CUSTOMER_ID \\033[0m Not good for segmentation. Afer analysis, we did not find a good split using this variable.\")\n",
    "print (\"     \")\n",
    "print(\"\\033[1m PROGRAM_NAME \\033[0m Not good for segmentation. Afer analysis, we did not find a good split using this variable.\")\n",
    "print (\"     \")\n",
    "print (\"\\033[1m LOAN_OPEN_DATE \\033[0m Good for segmentation.\")\n",
    "print(\"     Segment1: MONTHS_LOAN_OPENED < '12' [Accuracy Full Model: 80% / Accuracy Segmented Model: 80%]\")\n",
    "print(\"     Segment2: MONTHS_LOAN_OPENED >= '12' [Accuracy Full Model: 88% / Accuracy Segmented Model: 88%]\")\n",
    "print (\"     \")\n",
    "print(\"\\033[1m EXPECTED_CLOSE_DATE \\033[0m Not good for segmentation. Afer analysis, we did not find a good split using this variable.\")\n",
    "print (\"     \")\n",
    "print (\"\\033[1m ORIGINAL_BOOKED_AMOUNT \\033[0m Good for segmentation.\")\n",
    "print(\"     Segment1: ORIGINAL_BOOKED_AMOUNT < '1.000.000' [Accuracy Full Model: 99% / Accuracy Segmented Model: 99%]\")\n",
    "print(\"     Segment2: ORIGINAL_BOOKED_AMOUNT >= '1.000.000' [Accuracy Full Model: 83% / Accuracy Segmented Model: 83%]\")\n",
    "print (\"     \")\n",
    "print(\"\\033[1m OUTSTANDING \\033[0m Not good for segmentation. Afer analysis, we did not find a good split using this variable.\")\n",
    "print (\"     \")\n",
    "print (\"\\033[1m SEX \\033[0m Not good for segmentation. Afer analysis, we did not find a good split using this variable.\")\n",
    "print (\"     \")\n",
    "print (\"\\033[1m CUSTOMER_OPEN_DATE \\033[0m Good for segmentation.\")\n",
    "print(\"     Segment1: MONTHS_CLIENT_OPENED < '12' [Accuracy Full Model: 80% / Accuracy Segmented Model: 80%]\")\n",
    "print(\"     Segment2: MONTHS_CLIENT_OPENED >= '12' [Accuracy Full Model: 88% / Accuracy Segmented Model: 88%]\")\n",
    "print (\"     \")\n",
    "print (\"\\033[1m BIRTH_DATE \\033[0m Good for segmentation.\")\n",
    "print(\"     Segment1: AGE < '65' [Accuracy Full Model: 87% / Accuracy Segmented Model: 87%]\")\n",
    "print(\"     Segment2: AGE >= '65' [Accuracy Full Model: 83% / Accuracy Segmented Model: 83%]\")\n",
    "print (\"     \")\n",
    "print(\"\\033[1m PROFESSION \\033[0m Not good for segmentation. Afer analysis, we did not find a good split using this variable.\")\n",
    "print (\"     \")\n",
    "print(\"\\033[1m CAR_TYPE \\033[0m Not good for segmentation. Afer analysis, we did not find a good split using this variable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decission Trees Test #1 for the Group assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'bucket'\n",
    "all_variables = ['program_name', 'original_booked_amount', 'profession', 'car_type', 'user_type', 'age', 'months_loan_opened', 'months_to_close_loan', 'months_client_opened', 'F', 'M', 'outstanding_quantile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "splitter = train_test_split\n",
    "\"-----------------------\"\n",
    "\n",
    "df_train, df_test = splitter(df, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "X = df_train[all_variables]\n",
    "Y = df_train[target]\n",
    "\n",
    "#build decision tree\n",
    "clf = tree.DecisionTreeClassifier(criterion='gini', max_depth=4,min_samples_leaf=4)\n",
    "#max_depth represents max level allowed in each tree, min_samples_leaf minumum samples storable in leaf node\n",
    "\n",
    "#fit the tree to iris dataset\n",
    "clf.fit(X,Y)\n",
    "\n",
    "#plot decision tree\n",
    "fig, ax = plt.subplots(figsize=(100, 20)) #figsize value changes the size of plot\n",
    "tree.plot_tree(clf,ax=ax,feature_names= ['program_name', 'original_booked_amount', 'profession', 'car_type', 'user_type', 'age', 'months_loan_opened', 'months_to_close_loan', 'months_client_opened', 'F', 'M', 'outstanding_quantile'])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we can see in the decission tree above, the most relevant variables that affect our target variable \"BUCKET\" are:\n",
    "    #months loan opened (being the threshold 14.013)\n",
    "    #age( being the threshold 39.088)\n",
    "    #months_to_close_loan (being the threshold)\n",
    "    #car_type -> does not make sense since the threshold is established in 4.5 but it should be a categorical variable\n",
    "    #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting by months_loan_opened "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[all_variables]\n",
    "y_train = df_train[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test[all_variables]\n",
    "y_test = df_test[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "method = LogisticRegression(random_state=0)\n",
    "fitted_full_model = method.fit(X_train, y_train)\n",
    "y_pred = fitted_full_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_seg1 = df_train[df['months_loan_opened'] <14.013]\n",
    "df_train_seg2 = df_train[df['months_loan_opened'] >14.013]\n",
    "df_test_seg1 = df_test[df['months_loan_opened'] <14.013]\n",
    "df_test_seg2 = df_test[df['months_loan_opened'] >14.013]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seg1 = df_train_seg1[all_variables]\n",
    "y_train_seg1 = df_train_seg1[target]\n",
    "X_test_seg1 = df_test_seg1[all_variables]\n",
    "y_test_seg1 = df_test_seg1[target]\n",
    "fitted_model_seg1 = method.fit(X_train_seg1, y_train_seg1)\n",
    "\n",
    "def GINI(y_test, y_pred_probadbility):\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_probadbility)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    GINI = (2 * roc_auc) - 1\n",
    "    return(GINI)\n",
    "\n",
    "y_pred_seg1_proba = fitted_model_seg1.predict_proba(X_test_seg1)[:,1]\n",
    "y_pred_seg1_fullmodel_proba = fitted_full_model.predict_proba(X_test_seg1)[:,1]\n",
    "\n",
    "print(\"Segment1: months_loan_opened <14.013 [GINI Full Model: {:.4f}% / GINI Segmented Model: {:.4f}%]\".format(\n",
    "    GINI(y_test_seg1, y_pred_seg1_proba)*100,\n",
    "    GINI(y_test_seg1, y_pred_seg1_fullmodel_proba)*100\n",
    ")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seg2 = df_train_seg2[all_variables]\n",
    "y_train_seg2 = df_train_seg2[target]\n",
    "X_test_seg2 = df_test_seg2[all_variables]\n",
    "y_test_seg2 = df_test_seg2[target]\n",
    "fitted_model_seg2 = method.fit(X_train_seg2, y_train_seg2)\n",
    "\n",
    "def GINI(y_test, y_pred_probadbility):\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_probadbility)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    GINI = (2 * roc_auc) - 1\n",
    "    return(GINI)\n",
    "\n",
    "y_pred_seg2_proba = fitted_model_seg2.predict_proba(X_test_seg2)[:,1]\n",
    "y_pred_seg2_fullmodel_proba = fitted_full_model.predict_proba(X_test_seg2)[:,1]\n",
    "\n",
    "print(\"Segment2: months_loan_opened >14.013 [GINI Full Model: {:.4f}% / GINI Segmented Model: {:.4f}%]\".format(\n",
    "    GINI(y_test_seg2, y_pred_seg2_proba)*100,\n",
    "    GINI(y_test_seg2, y_pred_seg2_fullmodel_proba)*100\n",
    ")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting by age "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_seg1 = df_train[df['age'] <39.088]\n",
    "df_train_seg2 = df_train[df['age'] >39.088]\n",
    "df_test_seg1 = df_test[df['age'] <39.088]\n",
    "df_test_seg2 = df_test[df['age'] >39.088]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seg1 = df_train_seg1[all_variables]\n",
    "y_train_seg1 = df_train_seg1[target]\n",
    "X_test_seg1 = df_test_seg1[all_variables]\n",
    "y_test_seg1 = df_test_seg1[target]\n",
    "fitted_model_seg1 = method.fit(X_train_seg1, y_train_seg1)\n",
    "\n",
    "def GINI(y_test, y_pred_probadbility):\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_probadbility)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    GINI = (2 * roc_auc) - 1\n",
    "    return(GINI)\n",
    "\n",
    "y_pred_seg1_proba = fitted_model_seg1.predict_proba(X_test_seg1)[:,1]\n",
    "y_pred_seg1_fullmodel_proba = fitted_full_model.predict_proba(X_test_seg1)[:,1]\n",
    "\n",
    "print(\"Segment1: age <39 [GINI Full Model: {:.4f}% / GINI Segmented Model: {:.4f}%]\".format(\n",
    "    GINI(y_test_seg1, y_pred_seg1_proba)*100,\n",
    "    GINI(y_test_seg1, y_pred_seg1_fullmodel_proba)*100\n",
    ")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seg2 = df_train_seg2[all_variables]\n",
    "y_train_seg2 = df_train_seg2[target]\n",
    "X_test_seg2 = df_test_seg2[all_variables]\n",
    "y_test_seg2 = df_test_seg2[target]\n",
    "fitted_model_seg2 = method.fit(X_train_seg2, y_train_seg2)\n",
    "\n",
    "def GINI(y_test, y_pred_probadbility):\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_probadbility)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    GINI = (2 * roc_auc) - 1\n",
    "    return(GINI)\n",
    "\n",
    "y_pred_seg2_proba = fitted_model_seg2.predict_proba(X_test_seg2)[:,1]\n",
    "y_pred_seg2_fullmodel_proba = fitted_full_model.predict_proba(X_test_seg2)[:,1]\n",
    "\n",
    "print(\"Segment2: age >39 [GINI Full Model: {:.4f}% / GINI Segmented Model: {:.4f}%]\".format(\n",
    "    GINI(y_test_seg2, y_pred_seg2_proba)*100,\n",
    "    GINI(y_test_seg2, y_pred_seg2_fullmodel_proba)*100\n",
    ")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
